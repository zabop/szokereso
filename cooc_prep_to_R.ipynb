{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 32.7MB/s]                    \n",
      "2020-07-19 18:31:28 INFO: Downloading default packages for language: hu (Hungarian)...\n",
      "2020-07-19 18:31:29 INFO: File exists: /mnt/volume/jupyter/stanza_resources/hu/default.zip.\n",
      "2020-07-19 18:31:33 INFO: Finished downloading models and saved to /mnt/volume/jupyter/stanza_resources.\n",
      "2020-07-19 18:31:33 INFO: Loading these models for language: hu (Hungarian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | szeged  |\n",
      "| pos       | szeged  |\n",
      "| lemma     | szeged  |\n",
      "| depparse  | szeged  |\n",
      "=======================\n",
      "\n",
      "2020-07-19 18:31:33 INFO: Use device: cpu\n",
      "2020-07-19 18:31:33 INFO: Loading: tokenize\n",
      "2020-07-19 18:31:33 INFO: Loading: pos\n",
      "2020-07-19 18:31:35 INFO: Loading: lemma\n",
      "2020-07-19 18:31:35 INFO: Loading: depparse\n",
      "2020-07-19 18:31:36 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import stanza\n",
    "stanza.download('hu')\n",
    "nlp = stanza.Pipeline('hu')\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    if type(text) is float: return text\n",
    "    doc=nlp(text)\n",
    "    return ' '.join([word.lemma for word in doc.sentences[0].words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resfiles=glob.glob('resultfiles/*_szokereso_result.csv')\n",
    "dictstrings = ['person_data', 'wikilist','settlement_list','stanza','szotar_2.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res={}\n",
    "for resfile in resfiles:\n",
    "    df=pd.read_csv(resfile)\n",
    "    freqdf = pd.concat([df[col]\n",
    "                       for dictstring in dictstrings\n",
    "                       for col in df.columns if dictstring in col]\n",
    "                       ).value_counts(\n",
    "                       ).rename_axis('unique_entries_not_lemmatized'\n",
    "                       ).reset_index(name='counts')\n",
    "    res[resfile]=freqdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdf=res['resultfiles/data_2020-07-06_06:00:32_szokereso_result.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:41<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "freqdf['lemmatized']=freqdf.truncate(after=limit)['unique_entries_not_lemmatized'].progress_apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdf_onlylemmatized = freqdf.\\\n",
    "                        groupby(['lemmatized']).\\\n",
    "                        agg({'counts': 'sum'}).\\\n",
    "                        sort_values(by=['counts'],ascending=False).\\\n",
    "                        reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc = pd.DataFrame(index=freqdf_onlylemmatized['lemmatized'], columns=freqdf_onlylemmatized['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc.index.names =   ['row']\n",
    "cooc.columns.names = ['column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('resultfiles/data_2020-07-06_06:00:32_szokereso_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictcols = [col for col in df.columns if any([dictstring in col for dictstring in dictstrings])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.05\n",
      "0.1\n",
      "0.15\n",
      "0.2\n",
      "0.25\n",
      "0.3\n",
      "0.35\n",
      "0.4\n",
      "0.45\n",
      "0.5\n",
      "0.55\n",
      "0.6\n",
      "0.65\n",
      "0.7\n",
      "0.75\n",
      "0.8\n",
      "0.85\n",
      "0.9\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "for icoocrow, coocrow in enumerate(cooc.index):\n",
    "    for icooccol, cooccol in enumerate(cooc.columns):\n",
    "        if icoocrow ==icooccol: print(icoocrow/limit)\n",
    "        if icoocrow > icooccol:\n",
    "            count=0\n",
    "            for index, row in df[[*dictcols]].iterrows():\n",
    "                if cooccol in list(row) and coocrow in list(row):\n",
    "                    count+=1\n",
    "            cooc.loc[coocrow,cooccol]=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!RUN ONLY ONCE!!!\n",
    "cooc=cooc.add(cooc.transpose(),fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbán Viktor Varga Mihály 5\n",
      "Orbán Viktor Szijjártó Péter 7\n",
      "Orbán Viktor Karácsony Gergely 16\n",
      "Orbán Viktor Egyesült Államok 12\n",
      "Orbán Viktor Donald Trump 19\n",
      "Orbán Viktor Gulyás Gergely 2\n",
      "Orbán Viktor Hollik István 4\n",
      "Orbán Viktor Novák Katalin 2\n",
      "Orbán Viktor Bencsik János 2\n",
      "Varga Mihály Orbán Viktor 5\n",
      "Varga Mihály Szijjártó Péter 1\n",
      "Varga Mihály Karácsony Gergely 8\n",
      "Varga Mihály Tállai András 3\n",
      "Varga Mihály Gulyás Gergely 3\n",
      "Varga Mihály Hollik István 2\n",
      "Szijjártó Péter Orbán Viktor 7\n",
      "Szijjártó Péter Varga Mihály 1\n",
      "Szijjártó Péter Karácsony Gergely 2\n",
      "Szijjártó Péter Egyesült Államok 1\n",
      "Szijjártó Péter Novák Katalin 1\n",
      "Karácsony Gergely Orbán Viktor 16\n",
      "Karácsony Gergely Varga Mihály 8\n",
      "Karácsony Gergely Szijjártó Péter 2\n",
      "Karácsony Gergely Gulyás Gergely 4\n",
      "Karácsony Gergely Hollik István 16\n",
      "Karácsony Gergely Novák Katalin 1\n",
      "Karácsony Gergely Bencsik János 1\n",
      "Egyesült Államok Orbán Viktor 12\n",
      "Egyesült Államok Szijjártó Péter 1\n",
      "Egyesült Államok ##### EXTRA 1\n",
      "Egyesült Államok Donald Trump 21\n",
      "Egyesült Államok Tállai András 5\n",
      "##### EXTRA Egyesült Államok 1\n",
      "##### EXTRA Donald Trump 1\n",
      "##### EXTRA Schanda Tamás 1\n",
      "##### EXTRA Tállai András 1\n",
      "##### EXTRA Simon László 1\n",
      "Donald Trump Orbán Viktor 19\n",
      "Donald Trump Egyesült Államok 21\n",
      "Donald Trump ##### EXTRA 1\n",
      "Donald Trump Tállai András 5\n",
      "404 - article on site 109\n",
      "on site 404 - article 109\n",
      "Schanda Tamás ##### EXTRA 1\n",
      "Tállai András Varga Mihály 3\n",
      "Tállai András Egyesült Államok 5\n",
      "Tállai András ##### EXTRA 1\n",
      "Tállai András Donald Trump 5\n",
      "Gulyás Gergely Orbán Viktor 2\n",
      "Gulyás Gergely Varga Mihály 3\n",
      "Gulyás Gergely Karácsony Gergely 4\n",
      "Hollik István Orbán Viktor 4\n",
      "Hollik István Varga Mihály 2\n",
      "Hollik István Karácsony Gergely 16\n",
      "Novák Katalin Orbán Viktor 2\n",
      "Novák Katalin Szijjártó Péter 1\n",
      "Novák Katalin Karácsony Gergely 1\n",
      "Novák Katalin Varga György 2\n",
      "Varga György Novák Katalin 2\n",
      "Simon László ##### EXTRA 1\n",
      "Bencsik János Orbán Viktor 2\n",
      "Bencsik János Karácsony Gergely 1\n"
     ]
    }
   ],
   "source": [
    "f = open(\"toR2.txt\", \"w\")\n",
    "for i, col in enumerate(cooc.columns):\n",
    "    for j, index in enumerate(cooc.index):\n",
    "        if i!=j:\n",
    "            sig=cooc.loc[col,index]\n",
    "            if sig != 0:\n",
    "                print(col, index, sig)\n",
    "                f.write('|'.join([str(each) for each in [col, index, sig, '\\n']]))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
