{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import datetime\n",
    "import glob\n",
    "import stanza\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalenceClasses = {\n",
    "    'Fekete-Győr András': ['Fekete-Győr', 'Fekete Győr', 'Fekete-Győr András'],\n",
    "    'Borbélyné Bárdi Zsuzsanna': ['Borbélyné Bárdi Zsuzsanna'],\n",
    "    'Hajnal Miklós': ['Hajnal Miklós'],\n",
    "    'Körömi Attila': ['Körömi Attila','Körömi'],\n",
    "    'Mándi László': ['Mándi László','Mándi'],\n",
    "    'Orosz Anna': ['Orosz Anna'],\n",
    "    'Tóth Endre': ['Tóth Endre'],\n",
    "    'Cseh Katalin': ['Cseh Katalin'],\n",
    "    'Donáth Anna': ['Donáth Anna Júlia', 'Donáth Anna', 'Donáth'],\n",
    "    'Soproni Tamás': ['Soproni Tamás'],\n",
    "    'Déri Tibor': ['Déri Tibor'],\n",
    "    'Nyirati Klára': ['Nyirati Klára','Nyirati'],\n",
    "    'Pikó András': ['Pikó András', 'Pikó'],\n",
    "    'Balogh Csaba': ['Balogh Csaba'],\n",
    "    'Kerpel-Fronius Gábor': ['Kerpel-Fronius Gábor', 'Kerpel-Fronius'],\n",
    "    'Momentum': ['Momentum', 'momentum'],\n",
    "    'Kádár Barnabás': ['Kádár Barnabás'],\n",
    "    'Hadházy Ákos': ['Hadházy', 'Hadházy Ákos'],\n",
    "    'Szél Bernadett': ['Szél Bernadett']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "kezidict = set([\n",
    "'Fekete-Győr András',\n",
    "'Borbélyné Bárdi Zsuzsanna',\n",
    "'Hajnal Miklós',\n",
    "'Körömi Attila',\n",
    "'Mándi László',\n",
    "'Orosz Anna',\n",
    "'Tóth Endre',\n",
    "'Cseh Katalin',\n",
    "'Donáth Anna Júlia',\n",
    "'Soproni Tamás',\n",
    "'Déri Tibor',\n",
    "'Nyirati Klára',\n",
    "'Pikó András',\n",
    "'Balogh Csaba',\n",
    "'Kerpel-Fronius Gábor',\n",
    "'Momentum',\n",
    "'momentum',\n",
    "'Fekete-Győr',\n",
    "'Fekete Győr',\n",
    "'Donáth Anna',\n",
    "'Donáth',\n",
    "'Nyirati',\n",
    "'Pikó',\n",
    "'Kerpel-Fronius',\n",
    "'Mándi',\n",
    "'Körömi',\n",
    "'Kádár Barnabás',\n",
    "'Hadházy',\n",
    "'Hadházy Ákos',\n",
    "'Szél Bernadett',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_searchlist = [[each] for each in list(kezidict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlywithneighbours(ofthislist):\n",
    "    try:\n",
    "        filtered = [each for each in ofthislist \n",
    "                    if each+1 in ofthislist or each-1 in ofthislist]\n",
    "    except TypeError:\n",
    "        filtered = [each for each in ofthislist \n",
    "                    if str(int(each)+1) in ofthislist or str(int(each)-1) in ofthislist]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_numerical_sequences(inlist):\n",
    "\n",
    "    inlist=sorted(inlist)\n",
    "\n",
    "    breakindeces=[i for i,j in enumerate(inlist)\n",
    "                    if (j+1 not in inlist and j in inlist)]\n",
    "\n",
    "    sublists=[]\n",
    "    for index, each in enumerate(breakindeces):\n",
    "        if index==0:\n",
    "            sublists.append([x for x in inlist\n",
    "                               if x<=inlist[each]])\n",
    "        if index!=0:\n",
    "            sublists.append([x for x in inlist\n",
    "                               if x<=inlist[each] and x>inlist[breakindeces[index-1]]])\n",
    "\n",
    "    return sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validalias(alias):\n",
    "    return True\n",
    "   # if len(str(alias).strip()) > 5 and len(str(alias).strip().split(' ')) > 1: return True\n",
    "   # else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchfinder(text,searchforthese):\n",
    "    matches=[alias\n",
    "             for persondata_searchtarget in searchforthese\n",
    "             for alias in persondata_searchtarget\n",
    "             if validalias(alias) and alias.lower() in str(text).lower()]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillextracols(whichdictionary, whichrowtofill, withthislist, unlessitslongerthanthis):\n",
    "    if not len(withthislist) > unlessitslongerthanthis and len(withthislist)>0:\n",
    "        for i, e in enumerate(withthislist):\n",
    "            targetdf.loc[whichrowtofill,whichdictionary+str(i)]=e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_extra_columns(num_of_columns_for_each_dict):\n",
    "    for each in num_of_columns_for_each_dict:\n",
    "        for index in range(num_of_columns_for_each_dict[each]):\n",
    "            targetdf[each + str(index)]=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dictionary_class:\n",
    "    def __init__(self, name, maxcolnum, searchlist=None, geo=False):\n",
    "        self.name = name\n",
    "        self.maxcolnum = maxcolnum\n",
    "        self.searchlist = searchlist\n",
    "        self.geo = geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaries=[\n",
    "dictionary_class('momentumdict',\n",
    "           10,\n",
    "           momentum_searchlist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_sorted_by_date_after_a_date(look_for_this_pattern,cutoffdate):\n",
    "    \n",
    "    csvs = glob.glob(look_for_this_pattern)\n",
    "    datetimes=[datetime.datetime(int(each.split('/')[-1][5:9]),\n",
    "                             int(each.split('/')[-1][10:12]),\n",
    "                             int(each.split('/')[-1][13:15]),\n",
    "                             int(each.split('/')[-1][16:18]),\n",
    "                             int(each.split('/')[-1][19:21]),\n",
    "                             int(each.split('/')[-1][22:24])) for each in csvs]\n",
    "\n",
    "    dt_csvs_filtered=[[dt, csv] for dt, csv in zip(datetimes,csvs) if dt > datetime.datetime(*cutoffdate)]\n",
    "    \n",
    "    sorted_filtered_csvs = [csv\n",
    "                        for _, csv in sorted(\n",
    "                                         zip([eachpair[0] for eachpair in dt_csvs_filtered],\n",
    "                                             [eachpair[1] for eachpair in dt_csvs_filtered]))]\n",
    "    \n",
    "    return sorted_filtered_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap\n",
    "def equivalences(df, equivalenceClasses):\n",
    "    d = dict(ChainMap(*[dict.fromkeys(y,x) for x , y in equivalenceClasses.items()]))\n",
    "    df = df.replace(d)\n",
    "    df = df.mask(df.apply(pd.Series.duplicated,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugmode = False\n",
    "\n",
    "preMomFile = 'live_updated3_dict_onlymomentum_'\n",
    "postMomFileSzokereso = '_szokereso_result.csv'\n",
    "postMomFileSzurt = '_dfUnifiedNanFilteredOnlySomeColsUpdated3.csv'\n",
    "postMomFileErrorlog = '_ERRORLOG.txt'\n",
    "\n",
    "inputPathAndWildcard = '/mnt/volume/anagy/mediascraper/mediaScraper/output/data*csv'\n",
    "szokeresoResFilesPath =   '/mnt/volume/jupyter/szokereso/cron2/szokeresores/'\n",
    "szokeresoResFilesPathAndWildcard = szokeresoResFilesPath + preMomFile + '*' + postMomFileSzokereso\n",
    "momentumraSzurtSzokeresesPath    = szokeresoResFilesPath + 'momentumraszurt/'\n",
    "\n",
    "\n",
    "listOfColsWeWantToOutput = ['DOC_ID','TITLE','SCRAPETIME','TEXT','OUTLET','momentumdict0clssd', 'momentumdict1clssd',\n",
    "                            'momentumdict2clssd', 'momentumdict3clssd', 'momentumdict4clssd', 'momentumdict5clssd',\n",
    "                            'momentumdict6clssd', 'momentumdict7clssd', 'momentumdict8clssd', 'momentumdict9clssd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPathAndWildcard = '/mnt/volume/anagy/mediascraper/mediaScraper/output/data*csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputFiles(pathAndWildcard=szokeresoResFilesPathAndWildcard):\n",
    "    outputFiles = glob.glob(pathAndWildcard)\n",
    "    return outputFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTodoFilesWithoutOutput(todoFiles,outputFiles=getOutputFiles()):\n",
    "    return [todoFile for todoFile in todoFiles\n",
    "           if not any([todoFile.split('/')[-1].split('.')[0] in szokeresoOutput for szokeresoOutput in getOutputFiles()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doThisWhenNoFileIsFoundToProcess(sleepThisMuch=5*60):\n",
    "    print('No more files found to process. Sleeping '+str(sleepThisMuch)+' seconds.')\n",
    "    time.sleep(sleepThisMuch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: /mnt/volume/anagy/mediascraper/mediaScraper/output/data_2020-07-03_00:00:37.csv\n",
      "processing: /mnt/volume/anagy/mediascraper/mediaScraper/output/data_2020-07-03_02:00:33.csv\n",
      "processing: /mnt/volume/anagy/mediascraper/mediaScraper/output/data_2020-07-03_04:00:33.csv\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\n",
    "    todoFiles=get_files_sorted_by_date_after_a_date(inputPathAndWildcard,[2020,7,1,0])\n",
    "    todoFilesWithoutOutput = getTodoFilesWithoutOutput(todoFiles)\n",
    "    \n",
    "    if len(todoFilesWithoutOutput) == 0:\n",
    "        doThisWhenNoFileIsFoundToProcess()\n",
    "        continue\n",
    "        \n",
    "    print('processing: '+todoFilesWithoutOutput[0])    \n",
    "\n",
    "    targetcsv=todoFilesWithoutOutput[0]\n",
    "    try:\n",
    "        targetdf = pd.read_csv(targetcsv)\n",
    "        prepare_extra_columns({dictionary.name: dictionary.maxcolnum for dictionary in dictionaries})\n",
    "        for idictionary, dictionary in enumerate(dictionaries):\n",
    "            for icell, cell in enumerate(list(targetdf['TEXT'])):\n",
    "                if type(cell) is not float:\n",
    "                    if dictionary.searchlist is not None:\n",
    "                        fillextracols(dictionary.name,icell,matchfinder(cell,dictionary.searchlist),dictionary.maxcolnum)\n",
    "                    if dictionary.searchlist is None:\n",
    "                        fillextracols(dictionary.name,icell,stanzanamesearch(cell),dictionary.maxcolnum)\n",
    "        \n",
    "        filenamecore = targetcsv.split('/')[-1].split('.')[0]\n",
    "        szokeresoResFilePathAndName=szokeresoResFilesPath+preMomFile+filenamecore+postMomFileSzokereso\n",
    "        \n",
    "        if not debugmode:\n",
    "            targetdf.to_csv(szokeresoResFilePathAndName)\n",
    "        \n",
    "        df = pd.read_csv(szokeresoResFilePathAndName)\n",
    "        df = df[~df['momentumdict0'].isna()]\n",
    "        \n",
    "        classeddf = equivalences(df[[col for col in df.columns if 'momentum' in col]],equivalenceClasses)\n",
    "        classeddf.rename(columns={col:col+'clssd' for col in df.columns},inplace=True)\n",
    "        df = df.join(classeddf)\n",
    "        \n",
    "        if not debugmode: df[listOfColsWeWantToOutput].to_csv(momentumraSzurtSzokeresesPath+preMomFile+filenamecore+postMomFileSzurt)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except:\n",
    "        print('exception occured')\n",
    "        the_type, the_value, the_traceback = sys.exc_info()\n",
    "        outlist = [the_type, the_value, targetcsv, dictionary.name, icell]\n",
    "        if not debugmode:\n",
    "            with open(szokeresoResFilePathAndName+postMomFileErrorlog, 'w') as f:\n",
    "                for item in outlist:\n",
    "                    f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
