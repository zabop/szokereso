{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import datetime\n",
    "import glob\n",
    "import stanza\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filetime(file):\n",
    "    year=file.split('/')[-1][5:9]\n",
    "    month=file.split('/')[-1][10:12]\n",
    "    day=file.split('/')[-1][13:15]\n",
    "    hour=file.split('/')[-1][16:18]\n",
    "    return [int(each) for each in [year, month, day, hour]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 15.9MB/s]                    \n",
      "2020-07-13 21:25:36 INFO: Downloading default packages for language: hu (Hungarian)...\n",
      "2020-07-13 21:25:36 INFO: File exists: /mnt/volume/jupyter/stanza_resources/hu/default.zip.\n",
      "2020-07-13 21:25:40 INFO: Finished downloading models and saved to /mnt/volume/jupyter/stanza_resources.\n",
      "2020-07-13 21:25:40 INFO: Loading these models for language: hu (Hungarian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | szeged  |\n",
      "| pos       | szeged  |\n",
      "| lemma     | szeged  |\n",
      "| depparse  | szeged  |\n",
      "=======================\n",
      "\n",
      "2020-07-13 21:25:40 INFO: Use device: cpu\n",
      "2020-07-13 21:25:40 INFO: Loading: tokenize\n",
      "2020-07-13 21:25:40 INFO: Loading: pos\n",
      "2020-07-13 21:25:41 INFO: Loading: lemma\n",
      "2020-07-13 21:25:41 INFO: Loading: depparse\n",
      "2020-07-13 21:25:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('hu')\n",
    "nlp = stanza.Pipeline('hu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlywithneighbours(ofthislist):\n",
    "    try:\n",
    "        filtered = [each for each in ofthislist \n",
    "                    if each+1 in ofthislist or each-1 in ofthislist]\n",
    "    except TypeError:\n",
    "        filtered = [each for each in ofthislist \n",
    "                    if str(int(each)+1) in ofthislist or str(int(each)-1) in ofthislist]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_numerical_sequences(inlist):\n",
    "\n",
    "    inlist=sorted(inlist)\n",
    "\n",
    "    breakindeces=[i for i,j in enumerate(inlist)\n",
    "                    if (j+1 not in inlist and j in inlist)]\n",
    "\n",
    "    sublists=[]\n",
    "    for index, each in enumerate(breakindeces):\n",
    "        if index==0:\n",
    "            sublists.append([x for x in inlist\n",
    "                               if x<=inlist[each]])\n",
    "        if index!=0:\n",
    "            sublists.append([x for x in inlist\n",
    "                               if x<=inlist[each] and x>inlist[breakindeces[index-1]]])\n",
    "\n",
    "    return sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanzanamesearch(text):\n",
    "\n",
    "    try: doc = nlp(text)\n",
    "    except IndexError: return []\n",
    "    \n",
    "    propns_and_their_positions_dictlist = [{\n",
    "    #word.id:word.lemma\n",
    "    word.id:word.text\n",
    "    for word in sentence.words if word.upos == 'PROPN'}\n",
    "    for sentence in doc.sentences]\n",
    "\n",
    "    propns_with_at_least_one_propn_neighbour = [[\n",
    "    eachdict[eachkey]\n",
    "    for eachkey in sorted(onlywithneighbours(list(eachdict.keys())))]\n",
    "    for eachdict in propns_and_their_positions_dictlist]\n",
    "\n",
    "    propn_ids_with_at_least_one_propn_neighbour = [[\n",
    "    int(eachkey)\n",
    "    for eachkey in sorted(onlywithneighbours(list(eachdict.keys())))]\n",
    "    for eachdict in propns_and_their_positions_dictlist]\n",
    "\n",
    "    propn_id_sequences_with_at_least_one_propn_neighbour=[\n",
    "    split_into_numerical_sequences(eachsublist)\n",
    "    for eachsublist in propn_ids_with_at_least_one_propn_neighbour]\n",
    "\n",
    "    res=[]\n",
    "    for sentenceindex, eachsentence in enumerate(propn_id_sequences_with_at_least_one_propn_neighbour):\n",
    "        for sequenceindex, eachsequence in enumerate(eachsentence):\n",
    "            name=[]\n",
    "            for wordindex, eachword in enumerate(eachsequence):\n",
    "                name.append(propns_and_their_positions_dictlist[sentenceindex][str(eachword)])\n",
    "            name=' '.join(name)\n",
    "            res.append(name)\n",
    "\n",
    "    return list(set(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validalias(alias):\n",
    "    if len(alias.strip()) > 5 and len(alias.strip().split(' ')) > 1: return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchfinder(text,searchforthese):\n",
    "    matches=[alias\n",
    "             for persondata_searchtarget in searchforthese\n",
    "             for alias in persondata_searchtarget\n",
    "             if validalias(alias) and alias.lower() in str(text).lower()]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillextracols(whichdictionary, whichrowtofill, withthislist, unlessitslongerthanthis):\n",
    "    if not len(withthislist) > unlessitslongerthanthis and len(withthislist)>0:\n",
    "        for i, e in enumerate(withthislist):\n",
    "            targetdf.loc[whichrowtofill,whichdictionary+str(i)]=e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_extra_columns(num_of_columns_for_each_dict):\n",
    "    for each in num_of_columns_for_each_dict:\n",
    "        for index in range(num_of_columns_for_each_dict[each]):\n",
    "            targetdf[each + str(index)]=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dictionary_class:\n",
    "    def __init__(self, name, maxcolnum, searchlist=None, geo=False):\n",
    "        self.name = name\n",
    "        self.maxcolnum = maxcolnum\n",
    "        self.searchlist = searchlist\n",
    "        self.geo = geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchlist_maker(csv,excelfile=False,multisheet=False,headerwechoose='name_list',**kwargs):\n",
    "    \n",
    "    if not multisheet:\n",
    "        if excelfile: persondatadict = pd.read_excel(csv)\n",
    "\n",
    "        else:\n",
    "            persondatadict = pd.read_csv(csv)\n",
    "\n",
    "        if 'alias_separator' in kwargs:\n",
    "            persondata_searchlist = [[eachalias.strip()\n",
    "                                    for eachalias in eachperson.split(kwargs['alias_separator'])]\n",
    "                                    for eachperson in persondatadict[headerwechoose]\n",
    "                                    if type(eachperson) != float]\n",
    "        else:\n",
    "            persondata_searchlist = [[each]\n",
    "                                    for each in persondatadict[headerwechoose]]\n",
    "\n",
    "    if multisheet:\n",
    "        xls=pd.ExcelFile(csv)\n",
    "        dfs=[pd.read_excel(xls,sheet) for i, sheet in enumerate(xls.sheet_names)]\n",
    "        persondata_searchlist=[[name] for df in dfs for name in df['name_list'] if type(name) is not float]\n",
    "    \n",
    "    return persondata_searchlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaries=[\n",
    "dictionary_class('person_data',\n",
    "           10,\n",
    "           searchlist_maker('/mnt/volume/jupyter/szokereso/person_data-1592394309231_utf8.csv',alias_separator='|')),\n",
    "dictionary_class('wikilist',\n",
    "           5,\n",
    "           searchlist_maker('/mnt/volume/jupyter/szokereso/A_negyedik_OrbÃ¡n-kormany_allamtitkarainak_listaja.csv')),\n",
    "dictionary_class('stanza',\n",
    "           10),\n",
    "dictionary_class('settlement_list',\n",
    "           5,\n",
    "           searchlist_maker('/mnt/volume/jupyter/szokereso/List of Settlements_m2.xlsx', excelfile=True,headerwechoose='settlement_name'),\n",
    "           geo=True),\n",
    "dictionary_class('szotar_2.1',\n",
    "           10,\n",
    "           searchlist_maker('/mnt/volume/jupyter/szokereso/szotar_2.1.xlsx',multisheet=True))\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_sorted_by_date_after_a_date(look_for_this_pattern,cutoffdate):\n",
    "    \n",
    "    csvs = glob.glob(look_for_this_pattern)\n",
    "    datetimes=[datetime.datetime(int(each.split('/')[-1][5:9]),\n",
    "                             int(each.split('/')[-1][10:12]),\n",
    "                             int(each.split('/')[-1][13:15]),\n",
    "                             int(each.split('/')[-1][16:18]),\n",
    "                             int(each.split('/')[-1][19:21]),\n",
    "                             int(each.split('/')[-1][22:24])) for each in csvs]\n",
    "\n",
    "    dt_csvs_filtered=[[dt, csv] for dt, csv in zip(datetimes,csvs) if dt > datetime.datetime(*cutoffdate)]\n",
    "    \n",
    "    sorted_filtered_csvs = [csv\n",
    "                        for _, csv in sorted(\n",
    "                                         zip([eachpair[0] for eachpair in dt_csvs_filtered],\n",
    "                                             [eachpair[1] for eachpair in dt_csvs_filtered]))]\n",
    "    \n",
    "    return sorted_filtered_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugmode = False\n",
    "wildcard = '/mnt/volume/anagy/mediascraper/mediaScraper/output/data*csv'\n",
    "\n",
    "todofiles=get_files_sorted_by_date_after_a_date('/mnt/volume/anagy/mediascraper/mediaScraper/output/data*csv',\n",
    "                                               [2020,7,6,0])\n",
    "while len(todofiles) != 0:\n",
    "    print('processing: '+todofiles[0])    \n",
    "    \n",
    "    targetcsv=todofiles[0]\n",
    "    try:\n",
    "        targetdf = pd.read_csv(targetcsv)\n",
    "\n",
    "        prepare_extra_columns({dictionary.name: dictionary.maxcolnum for dictionary in dictionaries})\n",
    "\n",
    "        cells = list(targetdf['TEXT'])\n",
    "\n",
    "        start_time = time.time()\n",
    "        for idictionary, dictionary in enumerate(dictionaries):\n",
    "            if idictionary==2 or not debugmode:\n",
    "                for icell, cell in enumerate(cells):\n",
    "                    if type(cell) is not float:\n",
    "                        if icell > -1 or not debugmode:\n",
    "                            if icell%10==0: print(icell)\n",
    "                            if dictionary.searchlist is not None:\n",
    "                                fillextracols(dictionary.name,icell,matchfinder(cell,dictionary.searchlist),dictionary.maxcolnum)\n",
    "                            if dictionary.searchlist is None:\n",
    "                                fillextracols(dictionary.name,icell,stanzanamesearch(cell),dictionary.maxcolnum)\n",
    "        end_time = time.time()\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        targetdf.to_csv('/mnt/volume/jupyter/szokereso/resultfiles/'+targetcsv.split('/')[-1].split('.')[0]+'_szokereso_result.csv')\n",
    "        print(targetcsv)\n",
    "    except KeyboardInterrupt:\n",
    "        print('KeyboardInterrupt')\n",
    "        break\n",
    "    except:\n",
    "        the_type, the_value, the_traceback = sys.exc_info()\n",
    "        outlist = [the_type, the_value, targetcsv, dictionary.name, icell]\n",
    "        with open('/mnt/volume/jupyter/szokereso/resultfiles/'+targetcsv.split('/')[-1].split('.')[0]+'_ERRORLOG.txt', 'w') as f:\n",
    "            for item in outlist:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "                \n",
    "    todofiles=get_files_sorted_by_date_after_a_date('/mnt/volume/anagy/mediascraper/mediaScraper/output/data*csv',\n",
    "                                                     filetime(todofiles[0])[:3]+[filetime(todofiles[0])[-1]+1]) # hour increased by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
